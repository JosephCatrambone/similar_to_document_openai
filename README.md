## Overview

| Developed by | Guardrails AI |
| --- | --- |
| Date of development | Feb 15, 2024 |
| Validator type | Quality |
| Blog | - |
| License | Apache 2 |
| Input/Output | Output |

## Description

The objective of this validator is to ensure that any LLM-generated text is similar (in content) to a previously known document. This validator works comparing the LLM generated text with a known “good” document based on cosine similarity.

### Intended use

The primary intended uses of this validator is if a “golden” output is known for a similar subject when generating an LLM output. E.g., there’s previous historical data about a support QA system, and we want to ensure that any new LLM generated text will be similar to historical “golden” QA.

### Resources required

- Dependencies: Embedding model
- Foundation model access keys: Yes, if commercial embedding model used

## Installation

```bash
$ guardrails hub install hub://guardrails/similar_to_document

```

## Usage Examples

### Validating string output via Python

In this example, we apply the validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import SimilarToDocument
from guardrails import Guard

with open("/path/to/good/doc.txt", "r") as f:
    doc = f.read()

# Initialize Validator
val = SimilarToDocument(
    document=doc,
    threshold=0.8,
    model="text-embedding-ada-002"
)

# Setup Guard
guard = Guard.from_string(validators=[val])

guard.parse("LLM output here")
```

### Validating JSON output via Python

In this example, we apply the validator to a string field of a JSON output generated by an LLM.

```python
# Import Guard and Validator
from pydantic import BaseModel
from guardrails.hub import SimilarToDocument
from guardrails import Guard

with open("/path/to/good/doc.txt", "r") as f:
    doc = f.read()

# Initialize Validator
val = SimilarToDocument(
    document=doc,
    threshold=0.8,
    model="text-embedding-ada-002"
)

# Create Pydantic BaseModel
class LLMOutput(BaseModel):
    output: str = Field(
        description="LLM Output", validators=[val]
    )

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=LLMOutput)

# Run LLM output generating JSON through guard
guard.parse("""
{
    "output": "LLM output here",
}
""")
```

## API Reference

**`__init__(self, document, threshold=0.7, model="text-embedding-ada-002", on_fail="noop")`**
<ul>

Initializes a new instance of the Validator class.

**Parameters:**

- **`document`** _(str):_ The document to use for the similarity check.
- **`threshold`** _(float):_ The minimum cosine similarity to be considered similar. Defaults to 0.7.
- **`model`** _(str):_ The embedding model to use. Defaults to text-embedding-ada-002.
- **`on_fail`** *(str, Callable):* The policy to enact when a validator fails. If `str`, must be one of `reask`, `fix`, `filter`, `refrain`, `noop`, `exception` or `fix_reask`. Otherwise, must be a function that is called when the validator fails.

</ul>

<br>

**`__call__(self, value, metadata={}) → ValidationOutcome`**

<ul>

Validates the given `value` using the rules defined in this validator, relying on the `metadata` provided to customize the validation process. This method is automatically invoked by `guard.parse(...)`, ensuring the validation logic is applied to the input data.

Note:

1. This method should not be called directly by the user. Instead, invoke `guard.parse(...)` where this method will be called internally for each associated Validator.
2. When invoking `guard.parse(...)`, ensure to pass the appropriate `metadata` dictionary that includes keys and values required by this validator. If `guard` is associated with multiple validators, combine all necessary metadata into a single dictionary.

**Parameters:**

- **`value`** *(Any):* The input value to validate.
- **`metadata`** *(dict):* A dictionary containing metadata required for validation. No additional metadata keys are needed for this validator.

</ul>
